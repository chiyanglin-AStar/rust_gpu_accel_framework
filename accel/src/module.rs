//! CUDA Module (i.e. loaded PTX or cubin)

use crate::{contexted_call, contexted_new, device::*, error::*, *};
use cuda::*;
use std::{ffi::*, path::*, ptr::null_mut};

/// CUDA Kernel function
#[derive(Debug)]
pub struct Kernel<'module> {
    func: CUfunction,
    module: &'module Module,
}

impl Contexted for Kernel<'_> {
    fn sync(&self) -> Result<()> {
        self.module.context.sync()
    }

    fn version(&self) -> Result<u32> {
        self.module.context.version()
    }

    fn guard(&self) -> Result<ContextGuard> {
        self.module.context.guard()
    }
}

/// Type which can be sent to the device as kernel argument
///
/// ```
/// # use accel::*;
/// # use std::ffi::*;
/// let a: i32 = 10;
/// let p = &a as *const i32;
/// assert_eq!(
///     DeviceSend::as_ptr(&p),
///     &p as *const *const i32 as *mut c_void
/// );
/// assert!(std::ptr::eq(
///     unsafe { *(DeviceSend::as_ptr(&p) as *mut *const i32) },
///     p
/// ));
/// ```
pub trait DeviceSend: Sized {
    /// Get the address of this value
    fn as_ptr(&self) -> *mut c_void {
        self as *const Self as *mut c_void
    }
}

// Use default impl
impl<T> DeviceSend for *mut T {}
impl<T> DeviceSend for *const T {}
impl DeviceSend for bool {}
impl DeviceSend for i8 {}
impl DeviceSend for i16 {}
impl DeviceSend for i32 {}
impl DeviceSend for i64 {}
impl DeviceSend for isize {}
impl DeviceSend for u8 {}
impl DeviceSend for u16 {}
impl DeviceSend for u32 {}
impl DeviceSend for u64 {}
impl DeviceSend for usize {}
impl DeviceSend for f32 {}
impl DeviceSend for f64 {}

pub trait ArgRef {
    fn as_ptr(&self) -> *mut *mut c_void {
        self as *const Self as *mut *mut c_void
    }
}

// Generate `ArgRef*` structs like
//
// ```
// #[repr(C)]
// pub struct ArgRef2<'arg, D1: DeviceSend, D2: DeviceSend> {
//     pub arg1: &'arg D1,
//     pub arg2: &'arg D2,
// }
// ```
//
// and impls for `Send`, `Sync`, `From(&D1, &D2)`, and `ArgRef`
accel_derive::define_argref!(12 /* 1..=4 */);

/// Typed CUDA Kernel launcher
///
/// This will be automatically implemented in [accel_derive::kernel] for autogenerated wrapper
/// module of [Module].
///
/// ```
/// #[accel_derive::kernel]
/// fn f(a: i32) {}
/// ```
///
/// will create a submodule `f`:
///
/// ```
/// mod f {
///     pub const PTX_STR: &str = "PTX string generated by rustc/nvptx64-nvidia-cuda";
///     pub struct Module(::accel::Module);
///     /* impl Module { ... } */
///     /* impl Launchable for Module { ... } */
/// }
/// ```
///
/// Implementation of `Launchable` for `f::Module` is also generated by [accel_derive::kernel]
/// proc-macro.
///
/// [accel_derive::kernel]: https://docs.rs/accel-derive/0.3.0-alpha.1/accel_derive/attr.kernel.html
/// [Module]: struct.Module.html
pub trait Launchable<'arg> {
    /// Arguments for the kernel to be launched.
    /// This must be a tuple of [DeviceSend] types.
    ///
    /// [DeviceSend]: trait.DeviceSend.html
    type Args: ArgRef;

    fn get_kernel(&self) -> Result<Kernel>;

    /// Launch CUDA Kernel synchronously
    ///
    /// ```
    /// use accel::*;
    ///
    /// #[accel_derive::kernel]
    /// fn f(a: i32) {}
    ///
    /// let device = Device::nth(0)?;
    /// let ctx = device.create_context();
    /// let module = f::Module::new(&ctx)?;
    /// let a = 12;
    /// module.launch((1,) /* grid */, (4,) /* block */, (&a,))?; // wait until kernel execution ends
    /// # Ok::<(), ::accel::error::AccelError>(())
    /// ```
    fn launch<G: Into<Grid>, B: Into<Block>>(
        &self,
        grid: G,
        block: B,
        args: impl Into<Self::Args>,
    ) -> Result<()> {
        let grid = grid.into();
        let block = block.into();
        let kernel = self.get_kernel()?;
        let args: Self::Args = args.into();
        unsafe {
            contexted_call!(
                &kernel,
                cuLaunchKernel,
                kernel.func,
                grid.x,
                grid.y,
                grid.z,
                block.x,
                block.y,
                block.z,
                0,          /* FIXME: no shared memory */
                null_mut(), /* use default stream */
                args.as_ptr(),
                null_mut() /* no extra */
            )?;
        }
        kernel.sync()?;
        Ok(())
    }
}

/// OOP-like wrapper of `cuModule*` APIs
#[derive(Debug, Contexted)]
pub struct Module {
    module: CUmodule,
    context: Context,
}

impl Drop for Module {
    fn drop(&mut self) {
        if let Err(e) = unsafe { contexted_call!(&self.context, cuModuleUnload, self.module) } {
            log::error!("Failed to unload module: {:?}", e);
        }
    }
}

impl Module {
    /// integrated loader of Instruction
    pub fn load(context: &Context, data: &Instruction) -> Result<Self> {
        match *data {
            Instruction::PTX(ref ptx) => {
                let module =
                    unsafe { contexted_new!(context, cuModuleLoadData, ptx.as_ptr() as *const _)? };
                Ok(Module {
                    module,
                    context: context.clone(),
                })
            }
            Instruction::Cubin(ref bin) => {
                let module =
                    unsafe { contexted_new!(context, cuModuleLoadData, bin.as_ptr() as *const _)? };
                Ok(Module {
                    module,
                    context: context.clone(),
                })
            }
            Instruction::PTXFile(ref path) | Instruction::CubinFile(ref path) => {
                let filename = path_to_cstring(path);
                let module = unsafe { contexted_new!(context, cuModuleLoad, filename.as_ptr())? };
                Ok(Module {
                    module,
                    context: context.clone(),
                })
            }
        }
    }

    pub fn from_str(context: &Context, ptx: &str) -> Result<Self> {
        let data = Instruction::ptx(ptx);
        Self::load(context, &data)
    }

    /// Wrapper of `cuModuleGetFunction`
    pub fn get_kernel(&self, name: &str) -> Result<Kernel> {
        let name = CString::new(name).expect("Invalid Kernel name");
        let func =
            unsafe { contexted_new!(self, cuModuleGetFunction, self.module, name.as_ptr()) }?;
        Ok(Kernel { func, module: self })
    }
}

fn path_to_cstring(path: &Path) -> CString {
    CString::new(path.to_str().unwrap()).expect("Invalid Path")
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn load_do_nothing() -> Result<()> {
        // generated by do_nothing example in accel-derive
        let ptx = r#"
        .version 3.2
        .target sm_30
        .address_size 64
        .visible .entry do_nothing()
        {
          ret;
        }
        "#;
        let device = Device::nth(0)?;
        let ctx = device.create_context();
        let _mod = Module::from_str(&ctx, ptx)?;
        Ok(())
    }

    #[test]
    fn arg_ref2() {
        let a: i32 = 10;
        let b: f32 = 1.0;
        let args: ArgRef2<'_, i32, f32> = (&a, &b).into();
        let args_ptrs: &[*mut c_void] = unsafe { std::slice::from_raw_parts(args.as_ptr(), 2) };
        assert_eq!(
            args_ptrs,
            &[&a as *const _ as *mut c_void, &b as *const _ as *mut c_void]
        );
    }
}
